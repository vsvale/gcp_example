{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git -C ~ clone https://github.com/GoogleCloudPlatform/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export DP_STORAGE=\"gs://$(gcloud dataproc clusters describe sparktodp --region=us-central1 --format=json | jq -r '.config.configBucket')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp ~/training-data-analyst/quests/sparktobq/*.ipynb $DP_STORAGE/notebooks/jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/cloud-training/dataengineering/lab_assets/sparklab/kddcup.data_10_percent.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put kddcup* /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_file = \"hdfs:///kddcup.data_10_percent.gz\"\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "raw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\n",
    "parsed_rdd = csv_rdd.map(lambda r: Row(\n",
    "    duration=int(r[0]),\n",
    "    protocol_type=r[1],\n",
    "    service=r[2],\n",
    "    flag=r[3],\n",
    "    src_bytes=int(r[4]),\n",
    "    dst_bytes=int(r[5]),\n",
    "    wrong_fragment=int(r[7]),\n",
    "    urgent=int(r[8]),\n",
    "    hot=int(r[9]),\n",
    "    num_failed_logins=int(r[10]),\n",
    "    num_compromised=int(r[12]),\n",
    "    su_attempted=r[14],\n",
    "    num_root=int(r[15]),\n",
    "    num_file_creations=int(r[16]),\n",
    "    label=r[-1]\n",
    "    )\n",
    ")\n",
    "parsed_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(parsed_rdd)\n",
    "connections_by_protocol = df.groupBy('protocol_type').count().orderBy('count', ascending=False)\n",
    "connections_by_protocol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"connections\")\n",
    "attack_stats = sqlContext.sql(\"\"\"\n",
    "    SELECT\n",
    "      protocol_type,\n",
    "      CASE label\n",
    "        WHEN 'normal.' THEN 'no attack'\n",
    "        ELSE 'attack'\n",
    "      END AS state,\n",
    "      COUNT(*) as total_freq,\n",
    "      ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
    "      ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
    "      ROUND(AVG(duration), 2) as mean_duration,\n",
    "      SUM(num_failed_logins) as total_failed_logins,\n",
    "      SUM(num_compromised) as total_compromised,\n",
    "      SUM(num_file_creations) as total_file_creations,\n",
    "      SUM(su_attempted) as total_root_attempts,\n",
    "      SUM(num_root) as total_root_acceses\n",
    "    FROM connections\n",
    "    GROUP BY protocol_type, state\n",
    "    ORDER BY 3 DESC\n",
    "    \"\"\")\n",
    "attack_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PROJECT_ID=$(gcloud info --format='value(config.project)')\n",
    "!gsutil mb gs://$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/cloud-training/dataengineering/lab_assets/sparklab/kddcup.data_10_percent.gz\n",
    "!gsutil cp kddcup.data_10_percent.gz gs://$PROJECT_ID/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "gcs_bucket='[Your-Bucket-Name]'\n",
    "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_file = \"gs://\"+gcs_bucket+\"//kddcup.data_10_percent.gz\"\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "raw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile spark_analysis.py\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--bucket\", help=\"bucket for input and output\")\n",
    "args = parser.parse_args()\n",
    "BUCKET = args.bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_file = \"gs://{}/kddcup.data_10_percent.gz\".format(BUCKET)\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "#raw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_list = !gcloud info --format='value(config.project)'\n",
    "BUCKET=BUCKET_list[0]\n",
    "print('Writing to {}'.format(BUCKET))\n",
    "!/opt/conda/miniconda3/bin/python spark_analysis.py --bucket=$BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/sparktodp/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp spark_analysis.py gs://$BUCKET/sparktodp/spark_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil cp gs://$PROJECT_ID/sparktodp/spark_analysis.py spark_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano submit_onejob.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "gcloud dataproc jobs submit pyspark \\\n",
    "       --cluster sparktodp \\\n",
    "       --region us-central1 \\\n",
    "       spark_analysis.py \\\n",
    "       -- --bucket=$1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chmod +x submit_onejob.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./submit_onejob.sh $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
